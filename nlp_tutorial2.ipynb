{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><heading style=\"font-size:20px\"><i>This is the title<br><br></i></heading> <body><b>This is the body</b><p id=\"paral\">This is paral<a href=\"www.google.com\">Google</a></p> <p id=\"para2\">This is para 2</p></body></html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# lets create an example html to play with\n",
    "\n",
    "html =['<html><heading style=\"font-size:20px\"><i>This is the title<br><br></i></heading>',\n",
    "      '<body><b>This is the body</b><p id=\"paral\">This is paral<a href=\"www.google.com\">Google</a></p>',\n",
    "      '<p id=\"para2\">This is para 2</p></body></html>']\n",
    "html=' '.join(html)\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <heading style=\"font-size:20px\">\n",
      "   <i>\n",
      "    This is the title\n",
      "    <br/>\n",
      "    <br/>\n",
      "   </i>\n",
      "  </heading>\n",
      "  <b>\n",
      "   This is the body\n",
      "  </b>\n",
      "  <p id=\"paral\">\n",
      "   This is paral\n",
      "   <a href=\"www.google.com\">\n",
      "    Google\n",
      "   </a>\n",
      "  </p>\n",
      "  <p id=\"para2\">\n",
      "   This is para 2\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Indresh\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Indresh\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a soup object. This automatically identifies a structure in the html and creates a parse tree\n",
    "# we can navigate the structure/tree in the soup and extract pieces that you are interested in \n",
    "soup = BeautifulSoup(html)\n",
    "# You we now print this html in a formatted, easy-to-read view\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'html'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At the top of the hierarchy in the parse tree is the <html></html> tag\n",
    "# Then comes the <body></body> tag\n",
    "# Within the body, the heading and paragraphs are 'siblings'\n",
    "# The body is the parent of these tags and the html tag is the parent of the body tag \n",
    "# Each tag has attributes - name, contents (a list), text, parent and siblings \n",
    "\n",
    "# name attribute is just the name of the tag \n",
    "soup.html.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'body'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the title This is the bodyThis is paralGoogle This is para 2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text attribute will mush together all the text in all the children of that tag\n",
    "soup.body.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<body><heading style=\"font-size:20px\"><i>This is the title<br/><br/></i></heading> <b>This is the body</b><p id=\"paral\">This is paral<a href=\"www.google.com\">Google</a></p> <p id=\"para2\">This is para 2</p></body>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contents is a list of the children of that tag\n",
    "# In our example, the html tag has only 1 child, the body tag has 4 children \n",
    "soup.html.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<heading style=\"font-size:20px\"><i>This is the title<br/><br/></i></heading>,\n",
       " ' ',\n",
       " <b>This is the body</b>,\n",
       " <p id=\"paral\">This is paral<a href=\"www.google.com\">Google</a></p>,\n",
       " ' ',\n",
       " <p id=\"para2\">This is para 2</p>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'html'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parent and siblings referencing helps you navigate the parse tree\n",
    "soup.body.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"paral\">This is paral<a href=\"www.google.com\">Google</a></p>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.b.nextSibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b>This is the body</b>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.previousSibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<b>This is the body</b>]\n"
     ]
    }
   ],
   "source": [
    "# findAll, find are methods to search the tree for specific tags, or tags with certain attributes \n",
    "\n",
    "bold = soup.findAll('b')\n",
    "# This will find all the tags which have the text in bold (enclosed in <b></b> tags) and return a list\n",
    "print(bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the body\n"
     ]
    }
   ],
   "source": [
    "# to extract only the text, take each element of this list and get the text attribute\n",
    "print(bold[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is paralGoogle This is para 2\n",
      "[<p id=\"paral\">This is paral<a href=\"www.google.com\">Google</a></p>, <p id=\"para2\">This is para 2</p>]\n",
      "['This is paralGoogle', 'This is para 2']\n"
     ]
    }
   ],
   "source": [
    "# Let's get all the text that is in the paragraphs (enclosed in <p></p> tags) as a single string \n",
    "paras = ' '.join([p.text for p in soup.findAll('p')])\n",
    "print(paras)\n",
    "print(soup.findAll('p'))\n",
    "print([p.text for p in soup.findAll('p')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is para 2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# findAll can look for attributes as well. Let's find the text for the paragraph with id 'para2'\n",
    "soup.findAll(id='para2')[0].text\n",
    "#soup.findAll gives us a list, we pick the first element (there is only 1 element in this case) and print the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the title\n"
     ]
    }
   ],
   "source": [
    "# Let's find any text with font size 20\n",
    "font20=' '.join([p.text for p in soup.findAll(style=\"font-size:20px\")])\n",
    "print(font20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b>This is the body</b>,\n",
       " <p id=\"paral\">This is paral<a href=\"www.google.com\">Google</a></p>,\n",
       " <p id=\"para2\">This is para 2</p>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also pass in a list or a dictionary of tag names to search for \n",
    "soup.findAll(['b','p'])\n",
    "soup.findAll({'b':True,'p':True})\n",
    "# above both are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"www.google.com\">Google</a>]\n"
     ]
    }
   ],
   "source": [
    "# Let's see how we can find links. This is super-useful - say you want to find all the links on a page and iterate through\n",
    "# them to do some scraping for each of those links \n",
    "link = soup.findAll('a')\n",
    "\n",
    "# links are generally of the form <a href='link'>'link text'</a>\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notice how we used find instead of findAll - this will just give us the first tag that matches the search, in this\n",
    "# case we have only 1 link on our page. soup.findAll will return a list of links and you can limit the number of \n",
    "# results using the limit keyword soup.findAll('a',limit=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'get'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-9ea4a3ed8fd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Let's extract the url and the text separately\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(links['href']+\"is the url and\" +links.text + \" is the text\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" is the url and \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" is the text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#link.get('href')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1805\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1806\u001b[0m         raise AttributeError(\n\u001b[1;32m-> 1807\u001b[1;33m             \u001b[1;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1808\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'get'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "# Let's extract the url and the text separately \n",
    "#print(links['href']+\"is the url and\" +links.text + \" is the text\")\n",
    "print(link.get('href')+\" is the url and \"+link.text+\" is the text\")\n",
    "#link.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find can navigate the parse tree as well. findParents, findNextSiblings,findPreviousSiblings all work \n",
    "# similar to findAll, but will search only within those branches of the tree. \n",
    "# findNext, findPrevious and findAllNext and findAllPrevious can be used to find matches starting from \n",
    "# a specified point. \n",
    "\n",
    "# Let's say you want the text of the first paragraph after the first occurrence of the text \"Google\" \n",
    "soup.find(text='Google').findNext('p').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little shortcut to using findAll - if you call the tag itself as a function, you can use it in place of findAll\n",
    "# with the same arguments \n",
    "soup.body('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BeautifulSoup makes parsing html or xml very intuitive and elegant. Doing the same thing with regular expressions \n",
    "# is prone to leading you to pulling your hair out :) In most situations for screen-scraping projects, BeautifulSoup\n",
    "# is a life-saver! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
